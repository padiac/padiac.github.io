window.POSTS = [
  {
    slug: 'statistics-mvue-overview',
    title: 'Statistics - Minimum Variance Unbiased Estimator (MVUE)',
    date: '2025-02-19',
    category: 'Statistics',
    summary: 'Concise reference on MVUE definitions, existence, construction via Lehmann–Scheffé, classic normal examples, and ties to the Cramér–Rao bound.',
    content: '<h1>Minimum Variance Unbiased Estimator (MVUE)</h1><h2>1. Definition</h2><p>Let X = (X<sub>1</sub>, &hellip;, X<sub>n</sub>) be a random sample from a family of distributions { f(x; &theta;), &theta; &in; &Theta; }.</p><p>A statistic T(X) is called the <strong>Minimum Variance Unbiased Estimator (MVUE)</strong> of a function g(&theta;) if it satisfies:</p><ol><li><strong>Unbiasedness</strong><br />E<sub>&theta;</sub>[T(X)] = g(&theta;), &forall; &theta; &in; &Theta;.</li><li><strong>Minimum variance</strong><br />Among all unbiased estimators of g(&theta;), Var<sub>&theta;</sub>[T(X)] &le; Var<sub>&theta;</sub>[T&#771;(X)] for every alternative estimator T&#771;.</li></ol><hr /><h2>2. Existence and Uniqueness</h2><p>If an MVUE exists, it is <strong>unique</strong>.</p><p><em>Proof sketch:</em> If T<sub>1</sub> and T<sub>2</sub> are both MVUEs, then E<sub>&theta;</sub>[T<sub>1</sub> - T<sub>2</sub>] = 0 and Var<sub>&theta;</sub>[T<sub>1</sub> - T<sub>2</sub>] = 0, hence T<sub>1</sub> = T<sub>2</sub> almost surely.</p><hr /><h2>3. Finding the MVUE (Lehmann&ndash;Scheff&eacute; Theorem)</h2><p>The Lehmann&ndash;Scheff&eacute; theorem gives a constructive method to find MVUEs.</p><p><strong>Procedure:</strong></p><ol><li><strong>Find a sufficient statistic.</strong> Use the Fisher&ndash;Neyman factorization theorem to identify a sufficient statistic S(X).</li><li><strong>Check completeness.</strong> If S(X) is complete and sufficient, then:</li></ol><blockquote><p>For any unbiased estimator U(X) of g(&theta;), the conditional expectation T(X) = E[U(X) | S(X)] is the unique MVUE of g(&theta;).</p></blockquote><hr /><h2>4. Examples</h2><h3>Example 1: Mean of a Normal Distribution</h3><p>Let X<sub>1</sub>, &hellip;, X<sub>n</sub> &sim; N(&mu;, &sigma;<sup>2</sup>) with &sigma;<sup>2</sup> known.</p><p>The sample mean X&#772; = (1/n)&sum; X<sub>i</sub> is unbiased for &mu;.</p><p>Its variance is Var(X&#772;) = &sigma;<sup>2</sup> / n, which is the minimum possible among unbiased estimators.</p><p>&#9989; Therefore, X&#772; is the MVUE of &mu;.</p><h3>Example 2: Variance of a Normal Distribution</h3><p>Let X<sub>1</sub>, &hellip;, X<sub>n</sub> &sim; N(&mu;, &sigma;<sup>2</sup>).</p><p>With known &mu;, the unbiased estimator is S<sup>2</sup> = (1/n)&sum; (X<sub>i</sub> - &mu;)<sup>2</sup>, which is the MVUE of &sigma;<sup>2</sup>.</p><p><em>If &mu; is unknown:</em> the estimator &sigma;&#770;<sup>2</sup> = (1/(n - 1))&sum; (X<sub>i</sub> - X&#772;)<sup>2</sup> is unbiased and remains the MVUE of &sigma;<sup>2</sup>.</p><hr /><h2>5. Connection to the Cram&eacute;r&ndash;Rao Lower Bound (CRLB)</h2><p>For an unbiased estimator T(X) of g(&theta;), Var<sub>&theta;</sub>[T(X)] &ge; (g&#8242;(&theta;))<sup>2</sup> / I(&theta;), where I(&theta;) is the Fisher information.</p><p>If equality holds, the estimator is called <strong>efficient</strong>; it achieves the Cram&eacute;r&ndash;Rao lower bound and is therefore the MVUE.</p><hr /><h2>6. Summary Table</h2><table><thead><tr><th>Property</th><th>Meaning</th></tr></thead><tbody><tr><td>Unbiased</td><td>E[T] = g(&theta;)</td></tr><tr><td>Minimum Variance</td><td>No other unbiased estimator has smaller variance</td></tr><tr><td>Unique</td><td>If it exists, only one MVUE exists</td></tr><tr><td>Constructive Method</td><td>Use the Lehmann&ndash;Scheff&eacute; theorem</td></tr><tr><td>Relation to CRLB</td><td>Efficient estimators reach the CRLB and are MVUEs</td></tr></tbody></table><hr /><h2>7. References</h2><ul><li>E. L. Lehmann and G. Casella, <em>Theory of Point Estimation</em>. Springer, 1998.</li><li>G. Casella and R. L. Berger, <em>Statistical Inference</em>. Duxbury Press, 2002.</li><li>C. R. Rao, &ldquo;Information and Accuracy Attainable in the Estimation of Statistical Parameters.&rdquo; <em>Bulletin of the Calcutta Mathematical Society</em>, 1945.</li></ul>'
  },
  {
    slug: 'machine-learning-1d-yolo-baseline-aggregation',
    title: 'Machine Learning - 1D YOLO Baseline Aggregation Detector',
    date: '2025-02-18',
    category: 'Machine Learning',
    summary: 'Feasibility notes for framing baseline-to-aggregation detection as a 1D YOLO problem, covering model design, data synthesis, and upgrade paths.',
    content: '<p>This note captures a feasibility pass on adapting YOLO-style object detection to a 1D signal that exhibits a baseline segment followed by an aggregation ramp. The framing treats the composite structure as a temporal object whose center and segment lengths should be regressed alongside an objectness score.</p><h2>Problem Overview</h2><p>Given an observed signal <code>y(t)</code>, the detector should:</p><ol><li>Separate clean baseline-plus-aggregation patterns from pure noise.</li><li>Emit a tuple <code>(confidence, center, baseline_length, aggregation_length)</code> whenever a valid structure is detected.</li></ol><p>Effectively, we are transporting 2D object detection ideas into a 1D temporal domain.</p><h2>Why YOLO Works in 1D</h2><p>YOLO divides an input into anchor grids, predicts objectness, and regresses bounding boxes. In 1D, the same idea holds by sliding windows across time:</p><ul><li>Discretize the series into fixed-width cells.</li><li>Predict objectness plus localization parameters per cell.</li><li>Use non-max suppression or a simple confidence threshold to pick the best candidate.</li></ul><p>Comparable approaches are already effective in EEG event detection, seismic waveform picking, and time-series anomaly spotting. Papers like <em>YOLO-Time: A Unified Framework for Time-Series Object Detection</em> (2022) offer empirical backing.</p><h2>Lean Model Template</h2><ul><li><strong>Backbone:</strong> A 1D CNN with 3–5 convolutional blocks, optional residual skips, and dilations for receptive field growth.</li><li><strong>Detection head:</strong> Sigmoid for confidence, linear layers for <code>center</code>, <code>baseline_length</code>, <code>aggregation_length</code>.</li><li><strong>Loss:</strong> Binary cross-entropy on confidence plus Smooth L1 (Huber) on localization outputs.</li></ul><p>Keep the first pass lightweight to establish signal quality and viable hyperparameters.</p><h2>Synthetic Data Program</h2><p>Because real measurements are scarce, synthetic generation is the quickest path to a workable dataset:</p><ul><li>Sample baseline slopes, intercepts, and durations.</li><li>Attach aggregation ramps using exponential or logistic growth with randomized onset.</li><li>Add Gaussian or Poisson noise, random clipping, and occasional trend distortions.</li><li>Mix in pure noise sequences as negative examples.</li></ul><p>Augment with jittered phase shifts and amplitude scaling to encourage robustness before fine-tuning on real data.</p><h2>Extensions to Explore</h2><ul><li>Multi-anchor heads to support multiple simultaneous events.</li><li>Attention or Transformer blocks for longer-range temporal reasoning.</li><li>A DETR-style variant to drop anchors entirely and predict sets directly.</li><li>Confidence calibration (temperature scaling or Platt scaling) before deployment.</li></ul><h2>Key Takeaways</h2><table><thead><tr><th>Aspect</th><th>Assessment</th></tr></thead><tbody><tr><td>Problem Definition</td><td>Sound — baseline plus aggregation behaves like a composite object.</td></tr><tr><td>1D YOLO Fit</td><td>Feasible with precedent in time-series detection literature.</td></tr><tr><td>Primary Risk</td><td>Capturing realistic noise and drift in synthetic data.</td></tr><tr><td>Starter Recipe</td><td>Compact 1D CNN + multi-task head trained on curated synthetic signals.</td></tr><tr><td>Future Proofing</td><td>Scale to Transformer backbones or DETR for multi-event scenarios.</td></tr></tbody></table><h2>Suggested Next Steps</h2><ol><li>Prototype the 1D CNN + YOLO head in PyTorch.</li><li>Build a synthetic generator module with controllable noise and ramp parameters.</li><li>Stand up evaluation scripts that measure localization error, precision/recall, and confusion against negative samples.</li></ol>'
  },
  {
    slug: 'llm-prompt-chains-basics',
    title: 'LLM Engineering - Prompt Chains Basics',
    date: '2025-01-12',
    category: 'LLM Engineering',
    summary: 'Notes on structuring multi-step prompts, guarding failure modes, and adding evals.',
    content: '<p>This note outlines simple patterns for chaining prompts: classification -> extraction -> synthesis. Add lightweight checks at boundaries (for example, JSON schema validation) and track a few quality metrics for each step.</p><ul><li>Specify structured outputs (JSON with a schema)</li><li>Constrain temperature for extractive steps</li><li>Add small evals: completeness, format, contradiction</li></ul><p>Collect decision logs and surface failure examples to guide refinement.</p>'
  },
  {
    slug: 'stats-regression-diagnostics',
    title: 'Statistics - Regression Diagnostics Checklist',
    date: '2024-12-28',
    category: 'Statistics',
    summary: 'Quick checklist for linear model assumptions and what to do when they break.',
    content: '<p>Start with residual plots (trend, heteroscedasticity), influence (Cook\'s distance), multicollinearity (VIF), and distribution checks. If assumptions fail, consider transforms, robust standard errors, or alternative models (GLM, quantile regression).</p>'
  },
  {
    slug: 'learning-journal-ml-ai-foundations',
    title: 'Learning Journal — Machine Learning & AI Foundations',
    date: '2025-02-10',
    category: 'Statistics',
    summary: 'Curated roadmap of courses and books covering core ML theory, deep learning, and large-scale AI systems.',
    content: '<p>After research across platforms, I assembled this learning journal as a structured roadmap toward modern machine learning and large-scale AI systems. It mixes completed work with in-progress study notes.</p><h2>Courses</h2><h3>Deep Learning and Machine Learning</h3><ul><li><strong>Andrew Ng — Machine Learning (Stanford / Coursera)</strong>: Classical ML theory and supervised learning foundations. <em>Completed</em>.</li><li><strong>Andrew Ng — Convolutional Neural Networks (Deep Learning Specialization)</strong>: Core CNN architectures, pooling, and transfer learning. <em>Completed</em>.</li><li><strong>Andrew Ng — Sequence Models (Deep Learning Specialization #5)</strong>: RNN, LSTM, GRU, attention, and sequence modeling. <em>In progress (≈25%)</em>.</li><li><strong>Udemy — LLM Engineering: Master AI and Large Language Models</strong>: Building and deploying LLM applications. <em>Paused</em>.</li><li><strong>DeepLearning.AI — Natural Language Processing Specialization</strong>: Embeddings, sequence models, attention-based NLP pipelines. <em>Planned</em>.</li><li><strong>Hung-Yi Lee (NTU) — Machine Learning (2021)</strong>: Modern ML principles and interpretability. <em>Partially completed</em>.</li><li><strong>Hung-Yi Lee (NTU) — Generative AI (2023)</strong>: Generative model design and intuition. <em>Partially completed</em>.</li><li><strong>Deep Learning Specialization (supporting modules)</strong>: Neural Networks and Deep Learning, Improving Deep Neural Networks, Structuring ML Projects. <em>Reviewed selectively</em>.</li></ul><h2>Books & Technical References</h2><h3>Core Machine Learning and Mathematics</h3><ul><li><em>Machine Learning with PyTorch and Scikit-Learn</em> — Sebastian Raschka et al. <em>≈30% completed</em>.</li><li><em>Mathematics for Machine Learning</em> — Deisenroth, Faisal, Ong. <em>Referenced as needed</em>.</li><li><em>Statistics</em> (《数理统计》) — Xi-Ru Chen. <em>Currently studying</em>.</li></ul><h3>Advanced and Applied Topics</h3><ul><li><em>Generative AI System Design Interview</em>. <em>Next phase</em>.</li><li><em>Machine Learning System Design Interview</em>. <em>Next phase</em>.</li><li><em>Bayesian Analysis</em> — Lai-Sheng Wei, Wei-Ping Zhang. <em>Planned</em>.</li><li><em>Natural Language Processing with Transformers</em> — Tunstall, von Werra, Wolf. <em>Reference material</em>.</li></ul><p>This collection will keep evolving as I deepen both theoretical and engineering aspects of AI, from foundation math to LLM architectures.</p>'
  }
];
