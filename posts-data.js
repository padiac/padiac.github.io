window.POSTS = [
  {
    slug: 'statistics-mvue-overview',
    title: 'Statistics - Minimum Variance Unbiased Estimator (MVUE)',
    date: '2025-02-19',
    category: 'Statistics',
    summary: 'Concise reference on MVUE definitions, existence, construction via Lehmann–Scheffé, classic normal examples, and ties to the Cramér–Rao bound.',
    content: '<p>Let X = (X<sub>1</sub>, &hellip;, X<sub>n</sub>) denote a random sample from a family of distributions { f(x; &theta;) }. A statistic T(X) is the <strong>minimum variance unbiased estimator (MVUE)</strong> of a target function g(&theta;) when it delivers unbiased estimates and achieves the smallest possible variance among all unbiased estimators.</p><h2>Definition</h2><ul><li><strong>Unbiasedness:</strong> E<sub>&theta;</sub>[T(X)] = g(&theta;) for every parameter value.</li><li><strong>Minimum variance:</strong> Var<sub>&theta;</sub>[T(X)] is no larger than the variance of any competing unbiased estimator.</li></ul><h2>Existence and Uniqueness</h2><p>If an MVUE exists it is unique. Suppose T<sub>1</sub> and T<sub>2</sub> are both MVUEs. Their difference has zero expectation and zero variance, implying T<sub>1</sub> = T<sub>2</sub> with probability one.</p><h2>Lehmann–Scheff&eacute; Workflow</h2><ol><li>Identify a sufficient statistic S(X) using the Fisher–Neyman factorization criterion.</li><li>Show S(X) is complete. For exponential families this often holds automatically.</li><li>Take any unbiased estimator U(X) of g(&theta;), and compute T(X) = E[U(X) | S(X)]. That conditional expectation is the unique MVUE.</li></ol><h2>Examples</h2><h3>Normal Mean with Known Variance</h3><p>For X<sub>i</sub> &sim; N(&mu;, &sigma;<sup>2</sup>) with known &sigma;<sup>2</sup>, the sample mean &bar;X = (1/n) &sum; X<sub>i</sub> is unbiased for &mu; and has variance &sigma;<sup>2</sup>/n. No unbiased estimator can improve on that variance, so &bar;X is the MVUE of &mu;.</p><h3>Normal Variance</h3><p>With known &mu;, the statistic S<sup>2</sup> = (1/n) &sum; (X<sub>i</sub> - &mu;)<sup>2</sup> is unbiased for &sigma;<sup>2</sup> and serves as the MVUE. When &mu; is unknown, the corrected sample variance defined by (1/(n - 1)) &sum; (X<sub>i</sub> - &bar;X)<sup>2</sup> remains unbiased and is the MVUE.</p><h2>Connection to the Cram&eacute;r–Rao Bound</h2><p>Any unbiased estimator T(X) must satisfy Var<sub>&theta;</sub>[T(X)] &ge; (g&prime;(&theta;))<sup>2</sup> / I(&theta;), where I(&theta;) is the Fisher information. Estimators that meet this lower bound are termed <em>efficient</em>; they automatically qualify as MVUEs.</p><h2>Quick Summary</h2><table><thead><tr><th>Property</th><th>Meaning</th></tr></thead><tbody><tr><td>Unbiased</td><td>E[T] = g(&theta;)</td></tr><tr><td>Minimum variance</td><td>No other unbiased estimator has a smaller variance</td></tr><tr><td>Unique</td><td>Only one MVUE can exist</td></tr><tr><td>Construction</td><td>Complete, sufficient statistic + conditional expectation</td></tr><tr><td>CRLB link</td><td>Efficient estimators (achieve CRLB) are MVUEs</td></tr></tbody></table><h2>References</h2><ul><li>E. L. Lehmann and G. Casella, <em>Theory of Point Estimation</em>, Springer, 1998.</li><li>G. Casella and R. L. Berger, <em>Statistical Inference</em>, Duxbury Press, 2002.</li><li>C. R. Rao, &ldquo;Information and Accuracy Attainable in the Estimation of Statistical Parameters,&rdquo; <em>Bulletin of the Calcutta Mathematical Society</em>, 1945.</li></ul>'
  },
  {
    slug: 'machine-learning-1d-yolo-baseline-aggregation',
    title: 'Machine Learning - 1D YOLO Baseline Aggregation Detector',
    date: '2025-02-18',
    category: 'Machine Learning',
    summary: 'Feasibility notes for framing baseline-to-aggregation detection as a 1D YOLO problem, covering model design, data synthesis, and upgrade paths.',
    content: '<p>This note captures a feasibility pass on adapting YOLO-style object detection to a 1D signal that exhibits a baseline segment followed by an aggregation ramp. The framing treats the composite structure as a temporal object whose center and segment lengths should be regressed alongside an objectness score.</p><h2>Problem Overview</h2><p>Given an observed signal <code>y(t)</code>, the detector should:</p><ol><li>Separate clean baseline-plus-aggregation patterns from pure noise.</li><li>Emit a tuple <code>(confidence, center, baseline_length, aggregation_length)</code> whenever a valid structure is detected.</li></ol><p>Effectively, we are transporting 2D object detection ideas into a 1D temporal domain.</p><h2>Why YOLO Works in 1D</h2><p>YOLO divides an input into anchor grids, predicts objectness, and regresses bounding boxes. In 1D, the same idea holds by sliding windows across time:</p><ul><li>Discretize the series into fixed-width cells.</li><li>Predict objectness plus localization parameters per cell.</li><li>Use non-max suppression or a simple confidence threshold to pick the best candidate.</li></ul><p>Comparable approaches are already effective in EEG event detection, seismic waveform picking, and time-series anomaly spotting. Papers like <em>YOLO-Time: A Unified Framework for Time-Series Object Detection</em> (2022) offer empirical backing.</p><h2>Lean Model Template</h2><ul><li><strong>Backbone:</strong> A 1D CNN with 3–5 convolutional blocks, optional residual skips, and dilations for receptive field growth.</li><li><strong>Detection head:</strong> Sigmoid for confidence, linear layers for <code>center</code>, <code>baseline_length</code>, <code>aggregation_length</code>.</li><li><strong>Loss:</strong> Binary cross-entropy on confidence plus Smooth L1 (Huber) on localization outputs.</li></ul><p>Keep the first pass lightweight to establish signal quality and viable hyperparameters.</p><h2>Synthetic Data Program</h2><p>Because real measurements are scarce, synthetic generation is the quickest path to a workable dataset:</p><ul><li>Sample baseline slopes, intercepts, and durations.</li><li>Attach aggregation ramps using exponential or logistic growth with randomized onset.</li><li>Add Gaussian or Poisson noise, random clipping, and occasional trend distortions.</li><li>Mix in pure noise sequences as negative examples.</li></ul><p>Augment with jittered phase shifts and amplitude scaling to encourage robustness before fine-tuning on real data.</p><h2>Extensions to Explore</h2><ul><li>Multi-anchor heads to support multiple simultaneous events.</li><li>Attention or Transformer blocks for longer-range temporal reasoning.</li><li>A DETR-style variant to drop anchors entirely and predict sets directly.</li><li>Confidence calibration (temperature scaling or Platt scaling) before deployment.</li></ul><h2>Key Takeaways</h2><table><thead><tr><th>Aspect</th><th>Assessment</th></tr></thead><tbody><tr><td>Problem Definition</td><td>Sound — baseline plus aggregation behaves like a composite object.</td></tr><tr><td>1D YOLO Fit</td><td>Feasible with precedent in time-series detection literature.</td></tr><tr><td>Primary Risk</td><td>Capturing realistic noise and drift in synthetic data.</td></tr><tr><td>Starter Recipe</td><td>Compact 1D CNN + multi-task head trained on curated synthetic signals.</td></tr><tr><td>Future Proofing</td><td>Scale to Transformer backbones or DETR for multi-event scenarios.</td></tr></tbody></table><h2>Suggested Next Steps</h2><ol><li>Prototype the 1D CNN + YOLO head in PyTorch.</li><li>Build a synthetic generator module with controllable noise and ramp parameters.</li><li>Stand up evaluation scripts that measure localization error, precision/recall, and confusion against negative samples.</li></ol>'
  },
  {
    slug: 'llm-prompt-chains-basics',
    title: 'LLM Engineering - Prompt Chains Basics',
    date: '2025-01-12',
    category: 'LLM Engineering',
    summary: 'Notes on structuring multi-step prompts, guarding failure modes, and adding evals.',
    content: '<p>This note outlines simple patterns for chaining prompts: classification -> extraction -> synthesis. Add lightweight checks at boundaries (for example, JSON schema validation) and track a few quality metrics for each step.</p><ul><li>Specify structured outputs (JSON with a schema)</li><li>Constrain temperature for extractive steps</li><li>Add small evals: completeness, format, contradiction</li></ul><p>Collect decision logs and surface failure examples to guide refinement.</p>'
  },
  {
    slug: 'stats-regression-diagnostics',
    title: 'Statistics - Regression Diagnostics Checklist',
    date: '2024-12-28',
    category: 'Statistics',
    summary: 'Quick checklist for linear model assumptions and what to do when they break.',
    content: '<p>Start with residual plots (trend, heteroscedasticity), influence (Cook\'s distance), multicollinearity (VIF), and distribution checks. If assumptions fail, consider transforms, robust standard errors, or alternative models (GLM, quantile regression).</p>'
  },
  {
    slug: 'learning-journal-ml-ai-foundations',
    title: 'Learning Journal — Machine Learning & AI Foundations',
    date: '2025-02-10',
    category: 'Statistics',
    summary: 'Curated roadmap of courses and books covering core ML theory, deep learning, and large-scale AI systems.',
    content: '<p>After research across platforms, I assembled this learning journal as a structured roadmap toward modern machine learning and large-scale AI systems. It mixes completed work with in-progress study notes.</p><h2>Courses</h2><h3>Deep Learning and Machine Learning</h3><ul><li><strong>Andrew Ng — Machine Learning (Stanford / Coursera)</strong>: Classical ML theory and supervised learning foundations. <em>Completed</em>.</li><li><strong>Andrew Ng — Convolutional Neural Networks (Deep Learning Specialization)</strong>: Core CNN architectures, pooling, and transfer learning. <em>Completed</em>.</li><li><strong>Andrew Ng — Sequence Models (Deep Learning Specialization #5)</strong>: RNN, LSTM, GRU, attention, and sequence modeling. <em>In progress (≈25%)</em>.</li><li><strong>Udemy — LLM Engineering: Master AI and Large Language Models</strong>: Building and deploying LLM applications. <em>Paused</em>.</li><li><strong>DeepLearning.AI — Natural Language Processing Specialization</strong>: Embeddings, sequence models, attention-based NLP pipelines. <em>Planned</em>.</li><li><strong>Hung-Yi Lee (NTU) — Machine Learning (2021)</strong>: Modern ML principles and interpretability. <em>Partially completed</em>.</li><li><strong>Hung-Yi Lee (NTU) — Generative AI (2023)</strong>: Generative model design and intuition. <em>Partially completed</em>.</li><li><strong>Deep Learning Specialization (supporting modules)</strong>: Neural Networks and Deep Learning, Improving Deep Neural Networks, Structuring ML Projects. <em>Reviewed selectively</em>.</li></ul><h2>Books & Technical References</h2><h3>Core Machine Learning and Mathematics</h3><ul><li><em>Machine Learning with PyTorch and Scikit-Learn</em> — Sebastian Raschka et al. <em>≈30% completed</em>.</li><li><em>Mathematics for Machine Learning</em> — Deisenroth, Faisal, Ong. <em>Referenced as needed</em>.</li><li><em>Statistics</em> (《数理统计》) — Xi-Ru Chen. <em>Currently studying</em>.</li></ul><h3>Advanced and Applied Topics</h3><ul><li><em>Generative AI System Design Interview</em>. <em>Next phase</em>.</li><li><em>Machine Learning System Design Interview</em>. <em>Next phase</em>.</li><li><em>Bayesian Analysis</em> — Lai-Sheng Wei, Wei-Ping Zhang. <em>Planned</em>.</li><li><em>Natural Language Processing with Transformers</em> — Tunstall, von Werra, Wolf. <em>Reference material</em>.</li></ul><p>This collection will keep evolving as I deepen both theoretical and engineering aspects of AI, from foundation math to LLM architectures.</p>'
  }
];
