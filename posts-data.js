window.POSTS = [
  {
    slug: 'machine-learning-1d-yolo-baseline-aggregation',
    title: 'Machine Learning - 1D YOLO Baseline Aggregation Detector',
    date: '2025-02-18',
    category: 'Machine Learning',
    summary: 'Feasibility notes for framing baseline-to-aggregation detection as a 1D YOLO problem, covering model design, data synthesis, and upgrade paths.',
    content: '<p>This note captures a feasibility pass on adapting YOLO-style object detection to a 1D signal that exhibits a baseline segment followed by an aggregation ramp. The framing treats the composite structure as a temporal object whose center and segment lengths should be regressed alongside an objectness score.</p><h2>Problem Overview</h2><p>Given an observed signal <code>y(t)</code>, the detector should:</p><ol><li>Separate clean baseline-plus-aggregation patterns from pure noise.</li><li>Emit a tuple <code>(confidence, center, baseline_length, aggregation_length)</code> whenever a valid structure is detected.</li></ol><p>Effectively, we are transporting 2D object detection ideas into a 1D temporal domain.</p><h2>Why YOLO Works in 1D</h2><p>YOLO divides an input into anchor grids, predicts objectness, and regresses bounding boxes. In 1D, the same idea holds by sliding windows across time:</p><ul><li>Discretize the series into fixed-width cells.</li><li>Predict objectness plus localization parameters per cell.</li><li>Use non-max suppression or a simple confidence threshold to pick the best candidate.</li></ul><p>Comparable approaches are already effective in EEG event detection, seismic waveform picking, and time-series anomaly spotting. Papers like <em>YOLO-Time: A Unified Framework for Time-Series Object Detection</em> (2022) offer empirical backing.</p><h2>Lean Model Template</h2><ul><li><strong>Backbone:</strong> A 1D CNN with 3–5 convolutional blocks, optional residual skips, and dilations for receptive field growth.</li><li><strong>Detection head:</strong> Sigmoid for confidence, linear layers for <code>center</code>, <code>baseline_length</code>, <code>aggregation_length</code>.</li><li><strong>Loss:</strong> Binary cross-entropy on confidence plus Smooth L1 (Huber) on localization outputs.</li></ul><p>Keep the first pass lightweight to establish signal quality and viable hyperparameters.</p><h2>Synthetic Data Program</h2><p>Because real measurements are scarce, synthetic generation is the quickest path to a workable dataset:</p><ul><li>Sample baseline slopes, intercepts, and durations.</li><li>Attach aggregation ramps using exponential or logistic growth with randomized onset.</li><li>Add Gaussian or Poisson noise, random clipping, and occasional trend distortions.</li><li>Mix in pure noise sequences as negative examples.</li></ul><p>Augment with jittered phase shifts and amplitude scaling to encourage robustness before fine-tuning on real data.</p><h2>Extensions to Explore</h2><ul><li>Multi-anchor heads to support multiple simultaneous events.</li><li>Attention or Transformer blocks for longer-range temporal reasoning.</li><li>A DETR-style variant to drop anchors entirely and predict sets directly.</li><li>Confidence calibration (temperature scaling or Platt scaling) before deployment.</li></ul><h2>Key Takeaways</h2><table><thead><tr><th>Aspect</th><th>Assessment</th></tr></thead><tbody><tr><td>Problem Definition</td><td>Sound — baseline plus aggregation behaves like a composite object.</td></tr><tr><td>1D YOLO Fit</td><td>Feasible with precedent in time-series detection literature.</td></tr><tr><td>Primary Risk</td><td>Capturing realistic noise and drift in synthetic data.</td></tr><tr><td>Starter Recipe</td><td>Compact 1D CNN + multi-task head trained on curated synthetic signals.</td></tr><tr><td>Future Proofing</td><td>Scale to Transformer backbones or DETR for multi-event scenarios.</td></tr></tbody></table><h2>Suggested Next Steps</h2><ol><li>Prototype the 1D CNN + YOLO head in PyTorch.</li><li>Build a synthetic generator module with controllable noise and ramp parameters.</li><li>Stand up evaluation scripts that measure localization error, precision/recall, and confusion against negative samples.</li></ol>'
  },
  {
    slug: 'llm-prompt-chains-basics',
    title: 'LLM Engineering - Prompt Chains Basics',
    date: '2025-01-12',
    category: 'LLM Engineering',
    summary: 'Notes on structuring multi-step prompts, guarding failure modes, and adding evals.',
    content: '<p>This note outlines simple patterns for chaining prompts: classification -> extraction -> synthesis. Add lightweight checks at boundaries (for example, JSON schema validation) and track a few quality metrics for each step.</p><ul><li>Specify structured outputs (JSON with a schema)</li><li>Constrain temperature for extractive steps</li><li>Add small evals: completeness, format, contradiction</li></ul><p>Collect decision logs and surface failure examples to guide refinement.</p>'
  },
  {
    slug: 'stats-regression-diagnostics',
    title: 'Statistics - Regression Diagnostics Checklist',
    date: '2024-12-28',
    category: 'Statistics',
    summary: 'Quick checklist for linear model assumptions and what to do when they break.',
    content: '<p>Start with residual plots (trend, heteroscedasticity), influence (Cook\'s distance), multicollinearity (VIF), and distribution checks. If assumptions fail, consider transforms, robust standard errors, or alternative models (GLM, quantile regression).</p>'
  },
  {
    slug: 'learning-journal-ml-ai-foundations',
    title: 'Learning Journal — Machine Learning & AI Foundations',
    date: '2025-02-10',
    category: 'Statistics',
    summary: 'Curated roadmap of courses and books covering core ML theory, deep learning, and large-scale AI systems.',
    content: '<p>After research across platforms, I assembled this learning journal as a structured roadmap toward modern machine learning and large-scale AI systems. It mixes completed work with in-progress study notes.</p><h2>Courses</h2><h3>Deep Learning and Machine Learning</h3><ul><li><strong>Andrew Ng — Machine Learning (Stanford / Coursera)</strong>: Classical ML theory and supervised learning foundations. <em>Completed</em>.</li><li><strong>Andrew Ng — Convolutional Neural Networks (Deep Learning Specialization)</strong>: Core CNN architectures, pooling, and transfer learning. <em>Completed</em>.</li><li><strong>Andrew Ng — Sequence Models (Deep Learning Specialization #5)</strong>: RNN, LSTM, GRU, attention, and sequence modeling. <em>In progress (≈25%)</em>.</li><li><strong>Udemy — LLM Engineering: Master AI and Large Language Models</strong>: Building and deploying LLM applications. <em>Paused</em>.</li><li><strong>DeepLearning.AI — Natural Language Processing Specialization</strong>: Embeddings, sequence models, attention-based NLP pipelines. <em>Planned</em>.</li><li><strong>Hung-Yi Lee (NTU) — Machine Learning (2021)</strong>: Modern ML principles and interpretability. <em>Partially completed</em>.</li><li><strong>Hung-Yi Lee (NTU) — Generative AI (2023)</strong>: Generative model design and intuition. <em>Partially completed</em>.</li><li><strong>Deep Learning Specialization (supporting modules)</strong>: Neural Networks and Deep Learning, Improving Deep Neural Networks, Structuring ML Projects. <em>Reviewed selectively</em>.</li></ul><h2>Books & Technical References</h2><h3>Core Machine Learning and Mathematics</h3><ul><li><em>Machine Learning with PyTorch and Scikit-Learn</em> — Sebastian Raschka et al. <em>≈30% completed</em>.</li><li><em>Mathematics for Machine Learning</em> — Deisenroth, Faisal, Ong. <em>Referenced as needed</em>.</li><li><em>Statistics</em> (《数理统计》) — Xi-Ru Chen. <em>Currently studying</em>.</li></ul><h3>Advanced and Applied Topics</h3><ul><li><em>Generative AI System Design Interview</em>. <em>Next phase</em>.</li><li><em>Machine Learning System Design Interview</em>. <em>Next phase</em>.</li><li><em>Bayesian Analysis</em> — Lai-Sheng Wei, Wei-Ping Zhang. <em>Planned</em>.</li><li><em>Natural Language Processing with Transformers</em> — Tunstall, von Werra, Wolf. <em>Reference material</em>.</li></ul><p>This collection will keep evolving as I deepen both theoretical and engineering aspects of AI, from foundation math to LLM architectures.</p>'
  }
];
