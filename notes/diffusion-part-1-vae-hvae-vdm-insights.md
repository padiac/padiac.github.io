VAE & Variational Inference — Full Notes (Part 1)

> 本文档整合了我们此前所有讨论过的要点，包括：  
> * VAE 推导、ELBO 结构、KL ≥ 0、Jensen 不等式  
> * Z = 文字（latent = language）的解释  
> * μ / σ 的意义  
> * MLE 类比、为何需要 $q_\phi(z|x)$  
> * Decoder / Encoder 的作用  
> * 为什么无法直接算 $p(x)$  
> * 多步扩散的意义（为读 VDM 做准备）

---

## 1. 原始目标：我们想求什么？

我们有一个世界中的真实图像分布：

$$
p(x)
$$

这是所有图片的真实分布，但它无法直接计算，因为

$$
p(x) = \int p(x \mid z)p(z) dz.
$$

这里立刻面临两个困难：

1. 积分维度巨大（$z$ 的维度可达几百或几千）。
2. $p(z \mid x)$ 的真后验无法算（下一节展开）。


为了理解这里公式的意义和我们到底在优化什么，我们把问题形象化成如下版本： 
1. z 通常看成“抽象 latent”，但这里我们可以方便的认为就是图片对应的文字. $p(x \mid z)$ 是给定文字之后的图像分布, 如果分布已知，可以通过文字生成图像，这里x的自由度就是图片的dimension。例如每个pixel的值满足高斯分布，那么p的极大似然就是当前输入文字生成的图片。
3. $p(x, z)$ 是x和z的联合分布，也就是同一段文字可以生成无穷多图片（当然极大似然对应那个最优的结果），同样一张图片也可以用无穷多段文字描述。
4. 文字可以轻易的转化成embedding vector. 例如一个k dimension的向量空间，这时候应当认为这个空间是完备的，也就是文字可以取所有值。$p(z)$这时候就是先验分布。在贝叶斯中，我们有很多取先验的方式，其中正太分布是非常常见的一种, 下面的讨论中可以看到$p(z)$的确用的就是正态分布。比如如果认为X满足正太分布$N(\mu, \sigma^2)$, 那么在不知道$\mu$的情况下，我们会为它也选取一个正太分布$N(\tau, \theta^2)$
5. z不一定是文字表示，它可以是任何低维度的图片表示，这可以看成是原始图片的信息压缩。但也可以是相同size的噪声， VDM就是这个做法。
6. $p(x \mid z)$ 是给定文字之后的图像分布，也就是decoder. $p(z \mid x)$ 是给定图像之后的文字分布，也就是encoder. 这两个都是我们需要训练得到的结果，这个之后会细说。 

---

## 2. 真正的瓶颈：后验太难算

真实后验为

$$
p(z \mid x) = \frac{p(x \mid z)p(z)}{p(x)}.
$$

然而分母 $p(x)$ 要对所有 $z$ 积分，因此不可算，于是我们无法直接拿到真实的 posterior。

---

## 3. 关键思想：引入一个可控的替身 $q_\phi(z \mid x)$

我们构造一个人工后验

$$
q_\phi(z \mid x),
$$

它必须满足：

1. 我们能计算它。
2. 我们能从里面采样。
3. 它和真实后验越接近越好。

常见形式：

$$
q_\phi(z \mid x) = \mathcal{N}(z \mid \mu_\phi(x), \sigma_\phi^2(x) I).
$$

在上面的类比中：**$z$ 就是“文字”**；$\mu$ = 文字的中心位置，$\sigma$ = 该文字描述的模糊度。这两个参数其实正是我们想学的东西，如果z正好是是三维向量的话，它决定了每个图片都可以映射到三个“字”，这三个字就是图的低维表示。现在我们考虑一个比较简单的情况，就是我有两张图片，一张图片对应的文字是“一只猫”，另一张图片对应的文字是“一只老鼠”，这个时候你会觉得好像把猫和狗互换并没有任何影响。但其实是不对的，其实一开始真正的任务是给我们一个图片的training set，我们要学会这些图片的低维表示，或者这里就是文字表示。这意味着如果你有另外一张图片是“猫抓老鼠”的话，你学到到的意义就完全不对了。所以你要学的低维表示，它某种程度上是要“同构”于原本的图片，这才是我们学习的目的。


---

## 4. ELBO 推导, 从$\log p(x)$开始

我们从最原始的目标开始：最大化数据点 $x$ 的边缘似然 $\log p(x)$。因为

$$
p(x)=\int p(x,z) dz,
$$

这个积分在高维 latent 空间上无法解析，于是我们引入一个可控的近似后验 $q_\phi(z \mid x)$，把它强行塞进 $\log p(x)$：

$$
\log p(x)
= \log \int q_\phi(z \mid x)\frac{p(x \mid z)p(z)}{q_\phi(z \mid x)} dz.
$$


改写成期望形式：

$$
\log p(x) = \log E_{q_\phi(z \mid x)} \left[ \frac{p(x \mid z)p(z)} {q_\phi(z \mid x)} \right].
$$

现在出现关键结构 $\log E[\cdot]$，可用 Jensen 不等式：

$$
\log E[X] \ge E[\log X].
$$

于是得到 ELBO：

$$
\log p(x)
\ge 
E_{q_\phi(z \mid x)}
[\log p(x \mid z)]
-
\text{KL}(q_\phi(z \mid x)\Vert p(z)).
$$

完整写成：

$$
\text{ELBO}(x)
=
E_{q_\phi(z \mid x)}[\log p(x \mid z)]
-
\text{KL}(q_\phi(z \mid x)\Vert p(z)).
$$

为了看清楚这一分解从何而来，我们考虑真实后验的 KL：

$$
\text{KL}(q_\phi(z \mid x)\Vert p(z \mid x))
= 
E_{q_\phi}[\log q_\phi(z \mid x) - \log p(z \mid x)]
\ge 0.
$$

使用 Bayes 展开：

$$
\log p(z \mid x)
= \log p(x \mid z) + \log p(z) - \log p(x).
$$

代回 KL 展开式：

$$
\text{KL}
= 
E[\log q_\phi]
-
E[\log p(x \mid z)]
-
E[\log p(z)]
+ \log p(x).
$$

移项整理得：

$$
\log p(x)
=
E[\log p(x \mid z)]
-
\text{KL}(q_\phi(z \mid x)\Vert p(z))
+ 
\text{KL}(q_\phi(z \mid x)\Vert p(z \mid x)).
$$

由于 KL ≥ 0 (这里和Jensen 不等式只要二选一就能证明)，因此：

$$
\log p(x)\ge \text{ELBO}(x).
$$
开始

经典分解：

$$
\log p(x)
\ge E_{q_\phi(z \mid x)}[\log p(x \mid z)]
- \text{KL}(q_\phi(z \mid x) \Vert p(z)).
$$

右侧两项中：

- 第一项叫重构项，鼓励 decoder 生成像样的图片。
- 第二项是 KL 项，强制 encoder 输出的 $q_\phi(z \mid x)$ 靠近 prior $p(z)$。

---

## 6. 为什么我们最大化 ELBO？我们到底在优化什么？

首先，真实的数据分布 $p(x)$ 是一个真实存在的量，它既不可写出解析式，也不依赖任何我们可训练的参数。因此 $p(x)$ 本身无法直接“优化”。我们唯一能做的，是构造一个可计算的下界 —— ELBO，而 ELBO 的大小取决于我们学习的 $q_\phi(z \mid x)$ 和 $p(x \mid z)$。

从 ELBO 推导我们知道：

$$
\log p(x)
= E_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
- \mathrm{KL}\big(q_\phi(z\mid x)\Vert p(z)\big)
+ \mathrm{KL}\big(q_\phi(z\mid x)\Vert p(z\mid x)\big).
$$

注意这里的 **两个 KL 项不是同一个东西**：

1. 
$$
\mathrm{KL}\big(q_\phi(z\mid x)\Vert p(z)\big)
$$
这个是 ELBO 里面真正出现的那一项，它衡量的是：  
**我们学到的 posterior（encoder）与先验分布 $p(z)$ 的差异。**

2. 
$$
\mathrm{KL}\big(q_\phi(z\mid x)\Vert p(z\mid x)\big)
$$
这个 KL 是真实后验与我们学到的 posterior 之间的差异。  
它不在 ELBO 中出现，但它永远是非负的，用来建立下界关系。

因为第二个 KL（和真实后验的 KL）永远 ≥ 0，所以我们得到下界：

$$
\log p(x)
\ge 
E_{q_\phi}[\log p_\theta(x\mid z)]
-
\mathrm{KL}(q_\phi(z\mid x)\Vert p(z)).
$$

当且仅当

$$
q_\phi(z\mid x)=p(z\mid x)
$$

时，上述第二个 KL 等于 0，于是：

$$
\text{ELBO}(x)=\log p(x).
$$

也就是说：**只要我们学到的 $q_\phi(z\mid x)$ 完全等于真实后验，那么 ELBO 就等于真实但不可计算的 $\log p(x)$。**

---

然而，把 KL(q‖真实后验) 变成 0 的要求太高，因为真实 posterior 无法直接求解，因此我们只能最大化 ELBO 作为近似。这个过程本质上不是让某一项单独尽量大或尽量小，而是在以下两者之间取得平衡：

1. **重构项**  
$$
E_{q_\phi}[\log p_\theta(x\mid z)]
$$
希望 encoder 输出的 $z$ 包含足够的关于 $x$ 的信息，使 decoder 能尽量重建输入。

2. **KL 项（与先验的 KL）**  
$$
\mathrm{KL}(q_\phi(z\mid x)\Vert p(z))
$$
希望 latent 空间保持“规整”，让不同样本的 posterior 不至于发散，让整个 latent 空间保持可采样、结构良好。

如果我们让 KL(q‖p(z)) = 0（即 $q_\phi(z\mid x)=p(z)=\mathcal N(0,I)$），那么：

- encoder 必须输出 $\mu_\phi(x)=0,  \sigma_\phi(x)=1$
- 也就是说 **encoder 不再编码任何关于 x 的信息**
- 因此重构项会变得极差

所以虽然 KL(q‖p(z))=0 看上去很好，但它对应的是一个**完全无信息的 encoder**，导致 ELBO 反而极低。

相反，如果我们把重构项做到极大，让 encoder 完全自由地“塞满信息”，其结果往往会导致 KL(q‖p(z)) 非常大 —— latent 结构会塌缩成互不兼容的小岛，无法从 prior 随机采样也无法生成合理图像。

因此 ELBO 最大化的最优点并不是：

- KL = 0  
也不是  
- 重构误差 = 0  

而是：

$$
\text{ELBO 最大化点}
\quad=\quad
\text{重构足够好} + \text{latent 结构足够规整}
$$

在这个折中点上，我们得到最好的 $q_\phi(z\mid x)$：  
它既保留了图片的语义信息（你的“猫抓老鼠必须还是猫抓老鼠，而不是老鼠抓猫”），又保持 latent 空间和先验分布对齐，从而能进行生成、插值、采样等任务。

因此最大化 ELBO 的深层含义是：

**我们希望学到的 posterior 既能忠实表达输入的语义，又能嵌入一个统一、规整、可采样的 latent 空间中。**

这就是为什么 ELBO 是我们真正要优化的目标，而不是 $\log p(x)$。


再补充几点，在前面的分析中，我们说明了：最大化 ELBO 并不是直接最大化 $\log p(x)$，因为 $\log p(x)$ 是一个无法优化的常数；我们真正能优化、也真正想优化的是 **$q_\phi(z\mid x)$ 的质量** —— 即它是否成功学到图像的“低维语义表达”。然而，从数学结构上看，ELBO 的两项：

$$
E_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
\quad\text{与}\quad
\mathrm{KL}(q_\phi(z\mid x) \Vert p(z))
$$

都没有直接包含“语义”这一变量，因此 **ELBO 最大化为什么会让 latent 空间具有语义结构？** 这个问题并没有形式化的数学结论。

事实上，这一点属于**“涌现”**（emergence），而非严格定理。语义结构的出现来自三个间接但强力的机制：

---

### **（1）重构项迫使 $z$ 携带数据中最关键、最“信息压缩”的部分**

重构项  
$$
E_{q_\phi(z\mid x)}[\log p_\theta(x\mid z)]
$$
要求 decoder 必须仅凭 $z$ 重建出合理的图像。

由于 $z$ 的维度远小于 $x$，模型被迫把图像中最具区分度、最稳定、最“可压缩”的部分编码进去。在自然图像中，这些部分往往正对应语义内容（比如物体种类、位置、动作等）。因此 encoder 自然倾向于把“重要信息”挤进 latent 空间，而这些“重要信息”恰好就是语义。

---

### **（2）KL 项迫使所有后验贴近一个统一、光滑、连续的先验空间**

KL 项

$$
\mathrm{KL}(q_\phi(z\mid x)\Vert p(z))
$$

迫使所有 $q_\phi(z\mid x)$ 靠近一个简单分布（通常是标准高斯）。这带来两个深刻效果：

1. **连续性（continuity）**：相近的图片必须映射到相近的 latent 向量，否则 KL 会变大。  
2. **统一性（global consistency）**：不同图像不能各自占据孤岛式的 latent 区域，否则整个空间无法保持可采样性，KL 会强烈惩罚。

于是 latent 空间被迫形成一种**平滑、结构化的几何形状**，而这几何形状恰好往往对应图像语义的连续变化方式（例如从猫到老虎的连续变化）。

这种空间结构不是因为 KL 想要“语义”，而是因为 KL 想要“简单与平滑”，而自然图像的语义结构恰好也是一种平滑结构，于是两者天然契合。

---

### **（3）数据分布本身带有强烈的“语义结构”，模型的共享参数迫使它学出统一的编码体系**

自然图像有极强的统计结构：

- 有物体类别  
- 有共同形状  
- 有动作与姿态的连续变化  
- 有局部不变性、纹理分布、空间结构  
- 有明显的语义相似性（猫和老虎更近，猫和冰箱更远）

因为 encoder 和 decoder 是全局共享参数，它们必须用同一套编码规则处理所有图像。模型无法为每张图像发明独立、任意的 latent，因此唯一可行的策略就是：

> **让 latent 空间的几何结构匹配真实数据的语义结构。**

换言之：  
**语义之所以出现，是因为模型为了在有限容量下高效压缩必须捕捉数据本身的结构。**

---

### **总结：语义不是被“硬编码”出来的，而是被“逼出来”的**

ELBO 最大化本身并不保证语义结构，但以下三者的结合：

1. 重构项 —— 必须携带关键信息  
2. KL 项 —— 必须保持空间平滑与统一  
3. 数据结构 —— 自然图像本身具有语义规律  

共同导致 latent 空间形成类似“语言空间”的结构。

这是一种统计物理意义上的 *emergence*（涌现现象），  
而不是 VAE 架构中写死的数学定律。

这一点与你的直觉完全一致：  
**“模型似乎学会了语义，但它并非必然，这只是优化动力学 + 数据结构共同导致的副产物。”**

## 7. MLE 的类比

MLE 目标：

$$
\theta^\ast = \arg\max_\theta \prod_{i=1}^N p(x_i \mid \theta).
$$

VAE 目标：

$$
\max_{\phi, \theta} \sum_{i=1}^N \text{ELBO}(x_i).
$$

总结：

- 传统 MLE 对单个分布 $p(x \mid \theta)$ 寻找参数 $\theta$。
- VAE 对 encoder + decoder 这对模型寻找参数 $\phi, \theta$。
- 训练数据仍然独立同分布，因此要对所有样本求和。

你也敏锐地指出“应该出现 $N$ 张图的联合概率”，这是正确的；论文只是经常省略写法，本质就是 $\sum_i \text{ELBO}(x_i)$。

---

## 8. Decoder 的意义：$p_\theta(x \mid z)$

Decoder 通常建模为

$$
p_\theta(x \mid z) = \mathcal{N}(x \mid \mu_\theta(z), \sigma_\theta^2 I).
$$

你指出：

> Decoder 的 $\mu_\theta(z)$ 就是“给定文字生成图片的中心图像”，这是生成模型的关键直觉。

---

## 9. 多步扩散 vs 一步采样

虽然更偏向 VDM，但你的关键直觉值得记录：

> 理论上一大步可以折叠多小步，但多小步能让生成更稳定，因为每步加噪声会改变路径形状。

这正是 diffusion 相比 VAE 在图像生成上更强的重要原因之一；更完整的展开将在 VDM 部分记录。
